{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/train_V2.csv does not exist: '../input/train_V2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c26b0fce391e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#load train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[0mtrain_actual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/train_V2.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[0mtrain_actual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_actual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of train data: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_actual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/train_V2.csv does not exist: '../input/train_V2.csv'"
     ]
    }
   ],
   "source": [
    "# Load modules\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "#import lightgbm as lgb\n",
    "#from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#feature engineering - finding min, max and average grouped by match ID and group ID\n",
    "def feature_engineering(train):\n",
    "    drop_features_temp = ['Id', 'groupId', 'matchId', 'matchType']\n",
    "\n",
    "    features = list(set(train.columns) - set(drop_features_temp))\n",
    "\n",
    "    #min\n",
    "    min_group = train.groupby(by=[\"matchId\",\"groupId\"])[features].min()\n",
    "    min_group_rank = min_group.groupby('matchId')[features].rank(pct=True)\n",
    "    min_group = min_group.add_suffix('_min')\n",
    "    min_group_rank = min_group_rank.add_suffix('_min_rank')\n",
    "    print(\"added min features\")\n",
    "    #max\n",
    "    max_group = train.groupby(by=[\"matchId\",\"groupId\"])[features].max()\n",
    "    max_group_rank = max_group.groupby('matchId')[features].rank(pct=True)\n",
    "    max_group = max_group.add_suffix('_max')\n",
    "    max_group_rank = max_group_rank.add_suffix('_max_rank')\n",
    "    print(\"added max features\")\n",
    "    #mean\n",
    "    mean_group = train.groupby(by=[\"matchId\",\"groupId\"])[features].mean()\n",
    "    mean_group_rank = mean_group.groupby('matchId')[features].rank(pct=True)\n",
    "    mean_group = mean_group.add_suffix('_mean')\n",
    "    mean_group_rank = mean_group_rank.add_suffix('_mean_rank')\n",
    "    print(\"added mean features\")\n",
    "\n",
    "    grouped_train = pd.concat([min_group, min_group_rank, max_group, max_group_rank, mean_group, mean_group_rank], axis=1)\n",
    "    print(\"concatenated\")\n",
    "    del min_group, min_group_rank\n",
    "    del max_group, max_group_rank\n",
    "    del mean_group, mean_group_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return grouped_train\n",
    "\n",
    "\n",
    "#load train data\n",
    "train_actual = pd.read_csv(\"../input/train_V2.csv\")\n",
    "train_actual = train_actual.dropna()\n",
    "print(\"Number of train data: \", len(train_actual))\n",
    "\n",
    "#load test data\n",
    "test_actual = pd.read_csv(\"../input/test_V2.csv\")\n",
    "print(\"Number of test data: \", len(test_actual))\n",
    "\n",
    "#add extra features\n",
    "train_actual['headshotrate'] = train_actual['kills']/train_actual['headshotKills']\n",
    "train_actual['headshotrate'].fillna(0, inplace=True)\n",
    "train_actual['headshotrate'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['headshotrate'] = test_actual['kills']/test_actual['headshotKills']\n",
    "test_actual['headshotrate'].fillna(0, inplace=True)\n",
    "test_actual['headshotrate'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['killStreakrate'] = train_actual['killStreaks']/train_actual['kills']\n",
    "train_actual['killStreakrate'].fillna(0, inplace=True)\n",
    "train_actual['killStreakrate'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['killStreakrate'] = test_actual['killStreaks']/test_actual['kills']\n",
    "test_actual['killStreakrate'].fillna(0, inplace=True)\n",
    "test_actual['killStreakrate'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['healthitems'] = train_actual['heals'] + train_actual['boosts']\n",
    "train_actual['healthitems'].fillna(0, inplace=True)\n",
    "train_actual['healthitems'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['healthitems'] = test_actual['heals'] + test_actual['boosts']\n",
    "test_actual['healthitems'].fillna(0, inplace=True)\n",
    "test_actual['healthitems'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['totalDistance'] = train_actual['rideDistance'] + train_actual[\"walkDistance\"] + train_actual[\"swimDistance\"]\n",
    "test_actual['totalDistance'] = test_actual['rideDistance'] + test_actual[\"walkDistance\"] + test_actual[\"swimDistance\"]\n",
    "\n",
    "train_actual['killPlace_over_maxPlace'] = train_actual['killPlace'] / train_actual['maxPlace']\n",
    "train_actual['killPlace_over_maxPlace'].fillna(0, inplace=True)\n",
    "train_actual['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['killPlace_over_maxPlace'] = test_actual['killPlace'] / test_actual['maxPlace']\n",
    "test_actual['killPlace_over_maxPlace'].fillna(0, inplace=True)\n",
    "test_actual['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['headshotKills_over_kills'] = train_actual['headshotKills'] / train_actual['kills']\n",
    "train_actual['headshotKills_over_kills'].fillna(0, inplace=True)\n",
    "train_actual['headshotKills_over_kills'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['headshotKills_over_kills'] = test_actual['headshotKills'] / test_actual['kills']\n",
    "test_actual['headshotKills_over_kills'].fillna(0, inplace=True)\n",
    "test_actual['headshotKills_over_kills'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['distance_over_weapons'] = train_actual['totalDistance'] / train_actual['weaponsAcquired']\n",
    "train_actual['distance_over_weapons'].fillna(0, inplace=True)\n",
    "train_actual['distance_over_weapons'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['distance_over_weapons'] = test_actual['totalDistance'] / test_actual['weaponsAcquired']\n",
    "test_actual['distance_over_weapons'].fillna(0, inplace=True)\n",
    "test_actual['distance_over_weapons'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['walkDistance_over_heals'] = train_actual['walkDistance'] / train_actual['heals']\n",
    "train_actual['walkDistance_over_heals'].fillna(0, inplace=True)\n",
    "train_actual['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['walkDistance_over_heals'] = test_actual['walkDistance'] / test_actual['heals']\n",
    "test_actual['walkDistance_over_heals'].fillna(0, inplace=True)\n",
    "test_actual['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['walkDistance_over_kills'] = train_actual['walkDistance'] / train_actual['kills']\n",
    "train_actual['walkDistance_over_kills'].fillna(0, inplace=True)\n",
    "train_actual['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['walkDistance_over_kills'] = test_actual['walkDistance'] / test_actual['kills']\n",
    "test_actual['walkDistance_over_kills'].fillna(0, inplace=True)\n",
    "test_actual['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual['killsPerWalkDistance'] = train_actual['kills'] / train_actual['walkDistance']\n",
    "train_actual['killsPerWalkDistance'].fillna(0, inplace=True)\n",
    "train_actual['killsPerWalkDistance'].replace(np.inf, 0, inplace=True)\n",
    "test_actual['killsPerWalkDistance'] = test_actual['kills'] / test_actual['walkDistance']\n",
    "test_actual['killsPerWalkDistance'].fillna(0, inplace=True)\n",
    "test_actual['killsPerWalkDistance'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "train_actual[\"skill\"] = train_actual[\"headshotKills\"] + train_actual[\"roadKills\"]\n",
    "test_actual[\"skill\"] = test_actual[\"headshotKills\"] + test_actual[\"roadKills\"]\n",
    "\n",
    "#delete not required df\n",
    "del train_actual['heals'], test_actual['heals']\n",
    "gc.collect()\n",
    "\n",
    "#feature engineering\n",
    "train_actual_grouped = feature_engineering(train_actual)\n",
    "print(\"Number of train data grouped: \", len(train_actual_grouped))\n",
    "\n",
    "test_actual_grouped = feature_engineering(test_actual)\n",
    "print(\"Number of test data grouped: \", len(test_actual_grouped))\n",
    "\n",
    "# train, test split\n",
    "grouped_train, grouped_test = train_test_split(train_actual_grouped, test_size=0.2, random_state=0)\n",
    "print(\"Number of pseudo train data: \", len(grouped_train))\n",
    "print(\"Number of pseudo test data: \", len(grouped_test))\n",
    "\n",
    "# split features and target\n",
    "train_y = grouped_train['winPlacePerc_mean']\n",
    "train_x = grouped_train.drop(['winPlacePerc_min', 'winPlacePerc_min_rank','winPlacePerc_max','winPlacePerc_max_rank','winPlacePerc_mean', 'winPlacePerc_mean_rank'], axis=1)\n",
    "test_y = grouped_test[['winPlacePerc_mean']]\n",
    "test_x = grouped_test.drop(['winPlacePerc_min', 'winPlacePerc_min_rank','winPlacePerc_max','winPlacePerc_max_rank','winPlacePerc_mean', 'winPlacePerc_mean_rank'], axis=1)\n",
    "print(\"Number of train features: \", len(train_x.columns))\n",
    "print(\"Number of test features: \", len(test_x.columns))\n",
    "\n",
    "#delete not required df\n",
    "del train_actual_grouped, grouped_train, grouped_test\n",
    "gc.collect()\n",
    "\n",
    "#reduce memory\n",
    "train_x = reduce_mem_usage(train_x)\n",
    "test_x = reduce_mem_usage(test_x)\n",
    "\n",
    "#Light Gradient Boost Method\n",
    "train_data = lgb.Dataset(data=train_x, label=train_y)\n",
    "valid_data = lgb.Dataset(data=test_x, label=test_y)   \n",
    "params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':15000, 'early_stopping_rounds':100,\n",
    "          \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.9,\n",
    "           \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
    "         }\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=100) \n",
    "\n",
    "#predict for actual test grouped data\n",
    "test_actual_grouped_predict = lgb_model.predict(test_actual_grouped, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "#reduce memory\n",
    "test_actual_grouped = reduce_mem_usage(test_actual_grouped)\n",
    "test_actual = reduce_mem_usage(test_actual)\n",
    "\n",
    "#assign predictions\n",
    "test_actual_grouped[\"predict\"] = test_actual_grouped_predict\n",
    "\n",
    "#join predictions\n",
    "test_actual = test_actual.merge(test_actual_grouped[\"predict\"].reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "#re-assign predictions\n",
    "test_actual['winPlacePerc'] = test_actual['predict']\n",
    "\n",
    "#first submission\n",
    "submission1 = test_actual[['Id', 'winPlacePerc']]\n",
    "submission1.loc[submission1['winPlacePerc'] > 1.0, \"winPlacePerc\"] = 1.0\n",
    "submission1.loc[submission1['winPlacePerc'] < 0.0, \"winPlacePerc\"] = 0.0\n",
    "submission1.to_csv('submission1.csv', index=False)\n",
    "\n",
    "#copy first submission\n",
    "test_actual['winPlacePerc1'] = test_actual['winPlacePerc']\n",
    "\n",
    "#adjust predictions based on maxPlace\n",
    "test_actual['gap'] = 1.0 / (test_actual['maxPlace'] - 1)\n",
    "test_actual['winPlacePerc'] = round(test_actual['winPlacePerc']/ test_actual['gap']) * test_actual['gap']\n",
    "test_actual.loc[test_actual['winPlacePerc'] > 1.0, \"winPlacePerc\"] = 1.0\n",
    "test_actual.loc[test_actual['winPlacePerc'] < 0.0, \"winPlacePerc\"] = 0.0\n",
    "\n",
    "#second submission\n",
    "submission2 = test_actual[['Id', 'winPlacePerc']]\n",
    "submission2.to_csv('submission2.csv', index=False)\n",
    "\n",
    "#third submission\n",
    "df_sub = pd.read_csv(\"../input/sample_submission_V2.csv\")\n",
    "df_test = pd.read_csv(\"../input/test_V2.csv\")\n",
    "df_sub['winPlacePerc'] = test_actual['winPlacePerc1']\n",
    "\n",
    "# Restore some columns\n",
    "df_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\n",
    "\n",
    "# Sort, rank, and assign adjusted ratio\n",
    "df_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\n",
    "df_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\n",
    "df_sub_group = df_sub_group.merge(\n",
    "    df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n",
    "    on=\"matchId\", how=\"left\")\n",
    "df_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n",
    "\n",
    "df_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\n",
    "df_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n",
    "\n",
    "# Deal with edge cases\n",
    "df_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\n",
    "df_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n",
    "\n",
    "# Align with maxPlace\n",
    "# Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\n",
    "subset = df_sub.loc[df_sub.maxPlace > 1]\n",
    "gap = 1.0 / (subset.maxPlace.values - 1)\n",
    "new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n",
    "df_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n",
    "\n",
    "# Edge case\n",
    "df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\n",
    "assert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n",
    "\n",
    "df_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
